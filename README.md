# Task-2-rag_wikipedia-lab
# ğŸ“š Task 2: Wikipedia-based RAG Summarizer

## ğŸ¯ Overview

A Retrieval-Augmented Generation (RAG) system that extracts information from Wikipedia articles, creates semantic embeddings, stores them in a vector database, and enables intelligent querying about AI and healthcare topics.

**Framework**: LangChain + ChromaDB + SentenceTransformers  
**Data Source**: Wikipedia API  
**Embeddings**: all-MiniLM-L6-v2  
**No API Keys Required**: Runs completely locally in Google Colab

---

## ğŸ—ï¸ System Architecture

```
Wikipedia Articles
      â†“
Text Extraction & Chunking (~300 words)
      â†“
Semantic Embeddings (SentenceTransformers)
      â†“
Vector Storage (ChromaDB)
      â†“
Query Interface â†’ Similarity Search
      â†“
Retrieved Context â†’ Summary Generation
```

---

## ğŸ“‹ Features

- âœ… Fetches content from 4 Wikipedia articles on AI/Healthcare
- âœ… Chunks text into ~300-word segments for optimal retrieval
- âœ… Creates 384-dimensional semantic embeddings
- âœ… Stores in ChromaDB vector database with metadata
- âœ… Semantic similarity search (top-k retrieval)
- âœ… Generates coherent summaries from retrieved chunks
- âœ… Saves all outputs as CSV, JSON, and Markdown

---

## ğŸš€ Quick Start

### Prerequisites
- Google Colab account (free)
- No API keys required

### Installation (Cell 1)
```python
!pip install -q wikipedia-api sentence-transformers chromadb langchain pandas
```

### Run All Cells
Simply execute cells 1-14 in sequence. The notebook will:
1. Fetch Wikipedia data
2. Create embeddings
3. Build vector store
4. Run test queries
5. Generate final summary
6. Download all deliverables

---

## ğŸ“Š Wikipedia Topics Covered

| Topic | Description |
|-------|-------------|
| **Federated Learning** | Distributed machine learning for privacy preservation |
| **AI in Healthcare** | Medical applications of artificial intelligence |
| **Synthetic Data** | Artificially generated data for AI training |
| **Machine Learning** | Core ML concepts and techniques |

---

## ğŸ”§ Technical Components

### 1. Data Collection (Cell 4)
- Fetches Wikipedia articles using `wikipedia-api`
- Extracts full text content
- Chunks into ~300-word segments
- Saves to `wiki_corpus.csv`

### 2. Embedding Generation (Cell 5)
```python
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(texts)
```
- Model: all-MiniLM-L6-v2 (384 dimensions)
- Fast and accurate semantic embeddings
- Enables similarity-based retrieval

### 3. Vector Database (Cell 6)
```python
vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embeddings,
    metadatas=metadatas
)
```
- Database: ChromaDB (in-memory)
- Efficient similarity search
- Metadata preserved for source attribution

### 4. Query Pipeline (Cell 7-8)
- LangChain RetrievalQA chain
- Retrieves top 5 relevant chunks
- Combines context for comprehensive answers
- Returns sources with attribution

---

## ğŸ“ Deliverables

| File | Description | Format |
|------|-------------|--------|
| `wiki_corpus.csv` | Chunked Wikipedia data with metadata | CSV |
| `rag_summary.md` | Final 400-500 word research summary | Markdown |
| `retrieval_examples.json` | Sample queries with results | JSON |

---

## ğŸ” Sample Queries

The system can answer questions like:

```python
# Query 1
"What are the main challenges of federated learning in healthcare?"

# Query 2  
"How does synthetic data help in medical AI applications?"

# Query 3
"What are privacy concerns with federated learning?"

# Query 4
"Explain the benefits of federated learning"
```

---

## ğŸ“ˆ Performance Metrics

| Metric | Value |
|--------|-------|
| **Total Documents** | ~100-150 chunks |
| **Embedding Dimension** | 384 |
| **Retrieval Speed** | < 1 second |
| **Top-K Results** | 5 chunks per query |
| **Average Query Time** | 2-3 seconds |

---

## ğŸ“ How It Works

### Step 1: Wikipedia Extraction
```python
wiki = wikipediaapi.Wikipedia('en')
page = wiki.page("Federated_learning")
text = page.text
```

### Step 2: Text Chunking
- Splits articles into ~300 word segments
- Maintains context and readability
- Each chunk stored with metadata (title, URL, ID)

### Step 3: Semantic Embedding
- Converts text to 384-dimensional vectors
- Captures semantic meaning (not just keywords)
- Similar concepts cluster together in vector space

### Step 4: Vector Storage
- ChromaDB stores embeddings efficiently
- Enables fast similarity search
- Preserves metadata for source attribution

### Step 5: Query Processing
1. User asks a question
2. Question is embedded using same model
3. Vector similarity search finds top-K relevant chunks
4. Chunks are combined into context
5. Summary is generated from retrieved content

---

## ğŸ’¡ Key Advantages

### Retrieval-Augmented Generation
- âœ… **Factual Accuracy**: Answers grounded in Wikipedia sources
- âœ… **Source Attribution**: Every answer includes citations
- âœ… **No Hallucination**: Only uses retrieved information
- âœ… **Transparent**: Can inspect retrieved chunks

### vs. Traditional Search
- ğŸ” **Semantic**: Finds conceptually similar content, not just keywords
- ğŸ¯ **Context-Aware**: Understands meaning, not just word matching
- ğŸ“š **Comprehensive**: Combines multiple sources automatically

---

## ğŸ”¬ Use Cases

1. **Research Assistant**: Quick summaries of technical topics
2. **Educational Tool**: Learning about AI and healthcare
3. **Knowledge Base**: Query structured information efficiently
4. **Fact-Checking**: Verify claims against Wikipedia sources

---

## ğŸ› ï¸ Customization Options

### Change Topics
```python
TOPICS = [
    "Your_Topic_1",
    "Your_Topic_2",
    "Your_Topic_3"
]
```

### Adjust Chunk Size
```python
chunk_size = 300  # Change to 200, 400, etc.
```

### Modify Retrieval Count
```python
retriever=vectorstore.as_retriever(
    search_kwargs={"k": 10}  # Retrieve top 10 instead of 5
)
```

### Use Different Embedding Model
```python
model = SentenceTransformer("all-mpnet-base-v2")  # Larger, more accurate
```

---

## ğŸ“Š Evaluation Criteria

| Category | Points | Description |
|----------|--------|-------------|
| **Wikipedia Data** | 4 pts | Correct extraction and chunking |
| **Embedding + Storage** | 6 pts | SentenceTransformers + ChromaDB |
| **LangChain Pipeline** | 6 pts | Functional retrieval and generation |
| **Final Summary** | 4 pts | Coherence, accuracy, completeness |
| **Total** | **20 pts** | Full task completion |

---

## ğŸ› Troubleshooting

### Issue: Wikipedia fetch fails
**Solution**: Check internet connection, try different topics

### Issue: Out of memory
**Solution**: Reduce chunk size or number of topics

### Issue: Slow embeddings
**Solution**: Normal for CPU - GPU would be faster but not required

### Issue: ChromaDB errors
**Solution**: Restart runtime and run all cells fresh

---

## ğŸ“š Technologies Used

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.10+ | Programming language |
| **wikipedia-api** | Latest | Wikipedia content extraction |
| **sentence-transformers** | Latest | Semantic embeddings |
| **ChromaDB** | Latest | Vector database |
| **LangChain** | Latest | RAG pipeline orchestration |
| **Pandas** | Latest | Data manipulation |

---

## ğŸ¯ Learning Outcomes

After completing this task, you will understand:

- âœ… How RAG systems work
- âœ… Semantic search vs keyword search
- âœ… Vector embeddings and similarity
- âœ… Building knowledge bases from unstructured text
- âœ… LangChain retrieval patterns
- âœ… ChromaDB vector database operations

---

## ğŸ“– Additional Resources

- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [ChromaDB Guide](https://docs.trychroma.com/)
- [SentenceTransformers](https://www.sbert.net/)
- [Wikipedia API Docs](https://wikipedia-api.readthedocs.io/)

---

## ğŸ“ Next Steps

1. âœ… Complete Task 1 (Multi-Agent Research)
2. âœ… Complete Task 2 (This RAG system)
3. ğŸ“ Write comparison reflection (Bonus +3 pts)
4. ğŸ“¤ Submit both repositories

---

## ğŸ“ Support

If you encounter issues:
1. Check the troubleshooting section
2. Verify all cells ran in sequence
3. Restart runtime and try again
4. Review error messages carefully

---

## âœ¨ Success Criteria

Your Task 2 is complete when:
- âœ… All cells execute without errors
- âœ… `wiki_corpus.csv` contains 100+ chunks
- âœ… `rag_summary.md` has 400-500 words
- âœ… `retrieval_examples.json` has query results
- âœ… All files download successfully

